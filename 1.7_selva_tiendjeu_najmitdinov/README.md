# ğŸ“‰ Projet 1.7 (Sujet B) : Processus Gaussiens & Quantification d'Incertitude

> **Projet :** MSMIN5IN43 - IA probabiliste, thÃ©orie de jeux et machine learning  
> **Ã‰cole :** EPF Engineering School  
> **Auteur :** Selva Vicram, Tiendjeu Yannick, Najmitdinov AlekseÃ¯

Ce projet explore l'utilisation des **Processus Gaussiens (Gaussian Processes)** pour la rÃ©gression non-linÃ©aire et, surtout, pour la **quantification rigoureuse de l'incertitude** dans les modÃ¨les d'IA.

---

## ğŸ¯ Objectif du projet

Contrairement aux rÃ©seaux de neurones classiques qui donnent souvent une prÃ©diction ponctuelle avec une fausse confiance (overconfidence), ce projet vise Ã  construire un modÃ¨le "honnÃªte" capable de dire "Je ne sais pas".

* **ModÃ©lisation Non-LinÃ©aire :** Apprendre des fonctions complexes (ex: $x \sin(x)$) Ã  partir de peu de donnÃ©es.
* **Quantification d'Incertitude :** Distinguer les zones de certitude (proches des donnÃ©es) des zones d'inconnu (extrapolation).
* **Gestion du Bruit :** SÃ©parer le signal rÃ©el du bruit de mesure inhÃ©rent aux capteurs via un noyau composite.

### ğŸ’¡ Cas d'usage
* ğŸ¥ **MÃ©decine :** Diagnostiquer une maladie seulement si la certitude est > 99%.
* ğŸ’° **Finance :** Estimer non seulement le prix futur d'un actif, mais aussi le risque (volatilitÃ©) associÃ©.
* ğŸ¤– **Robotique :** Planification de trajectoire sÃ»re (Ã©viter les zones oÃ¹ le robot est "incertain" de l'environnement).

---

## ğŸ—ï¸ Architecture du projet
```plaintext
gp-uncertainty-project/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                 # ğŸš€ Script principal (Simulation & Visualisation)
â”‚   â””â”€â”€ presentation.ipynb      # ğŸ““ Notebook Jupyter pour la dÃ©mo interactive
â”œâ”€â”€ results/
â”‚   â””â”€â”€ resultat_gp.png         # ğŸ“Š Graphique de sortie (Preuve de concept)
â”œâ”€â”€ README.md                   # ğŸ“„ Ce fichier de documentation
â””â”€â”€ requirements.txt            # âœ… Liste des dÃ©pendances
```

---

## ğŸ§  Concepts MathÃ©matiques ClÃ©s

### 1. Le Processus Gaussien (GP)
Un GP n'apprend pas des paramÃ¨tres (poids), il apprend une distribution sur des fonctions.

Pour tout ensemble de points $x$, la distribution des sorties $f(x)$ est une gaussienne multivariÃ©e :

$$ f(x) \sim \mathcal{GP}(m(x), k(x, x')) $$

### 2. Le Noyau (Kernel) RBF
C'est le cÅ“ur du modÃ¨le. Il dÃ©finit la similaritÃ© entre deux points. Si deux points $x$ et $x'$ sont proches, leurs sorties $f(x)$ et $f(x')$ doivent Ãªtre corrÃ©lÃ©es.

$$ k(x, x') = \sigma_f^2 \exp\left(-\frac{(x-x')^2}{2l^2}\right) $$

* $l$ (length_scale) : ContrÃ´le la "douceur" de la fonction.
* $\sigma_f^2$ : ContrÃ´le l'amplitude verticale.

### 3. Gestion du Bruit (White Kernel)
Pour gÃ©rer des donnÃ©es rÃ©elles (bruitÃ©es), nous ajoutons un terme de bruit blanc Ã  la diagonale de la matrice de covariance :

$$ K_{y} = K_{f} + \sigma_n^2 I $$

Cela permet au modÃ¨le de ne pas sur-apprendre (overfit) le bruit des observations.

---

## âš™ï¸ Choix Techniques & Justification

Bien que le sujet suggÃ¨re initialement l'utilisation de GPyTorch (orientÃ© Deep Learning/GPU), nous avons fait un choix d'ingÃ©nierie diffÃ©rent pour ce cas d'usage prÃ©cis.

| CritÃ¨re | GPyTorch (SuggÃ©rÃ©) | Scikit-Learn (Choisi) |
|---------|-------------------|----------------------|
| **MÃ©thode de Calcul** | Approximation Variationnelle (VI) | InfÃ©rence Exacte (AlgÃ¨bre LinÃ©aire) |
| **Cible** | Big Data (> 100k points) | Small Data & PÃ©dagogie (< 1k points) |
| **PrÃ©cision** | DÃ©pend de l'optimisation | MathÃ©matiquement parfaite |
| **ComplexitÃ©** | Ã‰levÃ©e (Boucles d'optimisation manuelles) | Faible (API unifiÃ©e .fit()) |

**Verdict :** Pour une dÃ©monstration pÃ©dagogique sur la quantification d'incertitude, l'approche exacte de scikit-learn offre une visualisation plus rigoureuse et une meilleure explicabilitÃ© que les approximations nÃ©cessaires au passage Ã  l'Ã©chelle.

---

## ğŸš€ Installation & Utilisation

### PrÃ©requis : Python 3.8+

**Cloner le projet :**
```bash
git clone lien/du/projet
cd le-fichier
```

**Installer les dÃ©pendances :**
```bash
pip install numpy matplotlib scikit-learn
```

**Lancer la simulation :**
```bash
python src/main.py
```

Cela gÃ©nÃ©rera le graphique `resultat_gp.png` montrant la rÃ©gression et le tube d'incertitude.

---

## ğŸ› ï¸ Stack Technique

| CatÃ©gorie | Outils |
|-----------|--------|
| **ModÃ©lisation** | scikit-learn (GaussianProcessRegressor) |
| **Noyaux (Kernels)** | RBF, WhiteKernel |
| **Calcul Matriciel** | numpy |
| **Visualisation** | matplotlib |

---

## ğŸ“ˆ RÃ©sultats Obtenus

Le modÃ¨le parvient Ã  :

1. **Apprendre le signal :** La moyenne prÃ©dictive (ligne bleue) suit fidÃ¨lement la fonction cible $x\sin(x)$.
2. **Ignorer le bruit :** GrÃ¢ce au WhiteKernel, il ne "course" pas aprÃ¨s chaque point aberrant.
3. **Quantifier l'inconnu :** L'intervalle de confiance (zone bleue) s'Ã©largit drastiquement dans les zones sans donnÃ©es ($x > 10$), illustrant l'incertitude Ã©pistÃ©mique.

---

## ğŸ“ Contexte AcadÃ©mique

Ce projet est rÃ©alisÃ© dans le cadre du cours **IA Probabiliste (2025-2026)**.

* **Ã‰cole :** EPF Engineering School
* **Date de rendu :** 5 Janvier 2026
* **PrÃ©sentation :** 6 Janvier 2026

**CritÃ¨res de rÃ©ussite :**
* âœ… ComprÃ©hension des enjeux probabilistes.
* âœ… Justification pertinente des choix technologiques.
* âœ… QualitÃ© de la visualisation de l'incertitude.

---

## ğŸ“ Licence

MIT License.

Projet rÃ©alisÃ© pour l'exploration des modÃ¨les gÃ©nÃ©ratifs et probabilistes.